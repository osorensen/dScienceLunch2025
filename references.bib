@article{asparouhovDynamicStructuralEquation2018,
  title = {Dynamic {{Structural Equation Models}}},
  author = {Asparouhov, Tihomir and Hamaker, Ellen L. and Muth{\'e}n, Bengt},
  year = {2018},
  month = may,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {25},
  number = {3},
  pages = {359--388},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2017.1406803},
  abstract = {This article presents dynamic structural equation modeling (DSEM), which can be used to study the evolution of observed and latent variables as well as the structural equation models over time. DSEM is suitable for analyzing intensive longitudinal data where observations from multiple individuals are collected at many points in time. The modeling framework encompasses previously published DSEM models and is a comprehensive attempt to combine time-series modeling with structural equation modeling. DSEM is estimated with Bayesian methods using the Markov chain Monte Carlo Gibbs sampler and the Metropolis--Hastings sampler. We provide a detailed description of the estimation algorithm as implemented in the Mplus software package. DSEM can be used for longitudinal analysis of any duration and with any number of observations across time. Simulation studies are used to illustrate the framework and study the performance of the estimation method. Methods for evaluating model fit are also discussed.},
  keywords = {Baysian methods,dynamic factor analysis,intensive longitudinal data,time series analysis}
}

@article{burkeEcologicalMomentaryAssessment2017,
  title = {Ecological {{Momentary Assessment}} in {{Behavioral Research}}: {{Addressing Technological}} and {{Human Participant Challenges}}},
  shorttitle = {Ecological {{Momentary Assessment}} in {{Behavioral Research}}},
  author = {Burke, Lora E. and Shiffman, Saul and Music, Edvin and Styn, Mindi A. and Kriska, Andrea and Smailagic, Asim and Siewiorek, Daniel and Ewing, Linda J. and Chasens, Eileen and French, Brian and Mancino, Juliet and Mendez, Dara and Strollo, Patrick and Rathbun, Stephen L.},
  year = {2017},
  month = mar,
  journal = {Journal of Medical Internet Research},
  volume = {19},
  number = {3},
  pages = {e7138},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/jmir.7138},
  abstract = {Background: Ecological momentary assessment (EMA) assesses individuals' current experiences, behaviors, and moods as they occur in real time and in their natural environment. EMA studies, particularly those of longer duration, are complex and require an infrastructure to support the data flow and monitoring of EMA completion. Objective: Our objective is to provide a practical guide to developing and implementing an EMA study, with a focus on the methods and logistics of conducting such a study. Methods: The EMPOWER study was a 12-month study that used EMA to examine the triggers of lapses and relapse following intentional weight loss. We report on several studies that informed the implementation of the EMPOWER study: (1) a series of pilot studies, (2) the EMPOWER study's infrastructure, (3) training of study participants in use of smartphones and the EMA protocol and, (4) strategies used to enhance adherence to completing EMA surveys. Results: The study enrolled 151 adults and had 87.4\% (132/151) retention rate at 12 months. Our learning experiences in the development of the infrastructure to support EMA assessments for the 12-month study spanned several topic areas. Included were the optimal frequency of EMA prompts to maximize data collection without overburdening participants; the timing and scheduling of EMA prompts; technological lessons to support a longitudinal study, such as proper communication between the Android smartphone, the Web server, and the database server; and use of a phone that provided access to the system's functionality for EMA data collection to avoid loss of data and minimize the impact of loss of network connectivity. These were especially important in a 1-year study with participants who might travel. It also protected the data collection from any server-side failure. Regular monitoring of participants' response to EMA prompts was critical, so we built in incentives to enhance completion of EMA surveys. During the first 6 months of the 12-month study interval, adherence to completing EMA surveys was high, with 88.3\% (66,978/75,888) completion of random assessments and around 90\% (23,411/25,929 and 23,343/26,010) completion of time-contingent assessments, despite the duration of EMA data collection and challenges with implementation. Conclusions: This work informed us of the necessary preliminary steps to plan and prepare a longitudinal study using smartphone technology and the critical elements to ensure participant engagement in the potentially burdensome protocol, which spanned 12 months. While this was a technology-supported and -programmed study, it required close oversight to ensure all elements were functioning correctly, particularly once human participants became involved.},
  langid = {english}
}

@article{carneyPositiveNegativeDaily2000,
  title = {Positive and Negative Daily Events, Perceived Stress, and Alcohol Use: {{A}} Diary Study},
  shorttitle = {Positive and Negative Daily Events, Perceived Stress, and Alcohol Use},
  author = {Carney, Margaret Anne and Armeli, Stephen and Tennen, Howard and Affleck, Glenn and O'Neil, Timothy P.},
  year = {2000},
  journal = {Journal of Consulting and Clinical Psychology},
  volume = {68},
  number = {5},
  pages = {788--798},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2117},
  doi = {10.1037/0022-006X.68.5.788},
  abstract = {Using daily diary methodology, the authors examined over 60 days the within-person associations among positive and negative daily experiences, perceptions of stress, desire to drink, and alcohol consumption in a sample of 83 regular drinkers. Multilevel regression analyses indicated that days on which individuals reported more positive and negative nonwork events were also days they reported higher levels of desire to drink and actual consumption. Days on which individuals reported more negative work events were also days they reported a greater desire to drink, and days on which individuals reported more positive and negative health events were also days they reported lower levels of desire to drink and actual consumption. Weak evidence was found for the mediating effects of perceived stress in these associations. Several of the within-person associations varied as a function of gender, neuroticism, and drinking to cope; no moderating effects were found for extraversion. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Alcohol Use,Alcohol Use Attitudes,Life Experiences,Stress}
}

@article{carpenterStanProbabilisticProgramming2017,
  title = {Stan: {{A Probabilistic Programming Language}}},
  shorttitle = {Stan},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  month = jan,
  journal = {Journal of Statistical Software},
  volume = {76},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
  langid = {english},
  keywords = {algorithmic differentiation,Bayesian inference,probabilistic programming,Stan}
}

@article{hoffmanNoUTurnSamplerAdaptively2014,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {47},
  pages = {1593--1623},
  issn = {1533-7928},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size {$\epsilon\epsilon\backslash$}epsilon and a desired number of steps LLL. In particular, if LLL is too small then the algorithm exhibits undesirable random walk behavior, while if LLL is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps LLL. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter {$\epsilon\epsilon\backslash$}epsilon on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient {\^a}turnkey{\^a} samplers.}
}

@article{juddInterindividualDifferencesCognitive2024,
  title = {Interindividual {{Differences}} in {{Cognitive Variability Are Ubiquitous}} and {{Distinct From Mean Performance}} in a {{Battery}} of {{Eleven Tasks}}},
  author = {Judd, Nicholas and Aristodemou, Michael and Klingberg, Torkel and Kievit, Rogier},
  year = {2024},
  month = may,
  journal = {Journal of Cognition},
  volume = {7},
  number = {1},
  issn = {2514-4820},
  doi = {10.5334/joc.371},
  abstract = {The Journal of Cognition, the official journal of the European Society for Cognitive Psychology, publishes reviews, empirical articles (including registered reports), data reports, stimulus development reports, comments, and methodological notes relevant to all areas of cognitive psychology, including attention, memory, perception, psycholinguistics, and reasoning. We also publish cross-disciplinary research if we judge that it has clear implications for development of cognitive psychological theories. As a signatory of the Center for Open Science's Transparency and Openness Promotion guidelines, we value methodological rigour and transparent scientific practices. We welcome submissions from scholars working anywhere in the world.},
  langid = {american}
}

@article{mcneishPrimerTwolevelDynamic2020,
  title = {A Primer on Two-Level Dynamic Structural Equation Models for Intensive Longitudinal Data in {{Mplus}}},
  author = {McNeish, Daniel and Hamaker, Ellen L.},
  year = {2020},
  journal = {Psychological Methods},
  volume = {25},
  number = {5},
  pages = {610--635},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/met0000250},
  abstract = {Technological advances have led to an increase in intensive longitudinal data and the statistical literature on modeling such data is rapidly expanding, as are software capabilities. Common methods in this area are related to time-series analysis, a framework that historically has received little exposure in psychology. There is a scarcity of psychology-based resources introducing the basic ideas of time-series analysis, especially for data sets featuring multiple people. We begin with basics of N = 1 time-series analysis and build up to complex dynamic structural equation models available in the newest release of Mplus Version 8. The goal is to provide readers with a basic conceptual understanding of common models, template code, and result interpretation. We provide short descriptions of some advanced issues, but our main priority is to supply readers with a solid knowledge base so that the more advanced literature on the topic is more readily digestible to a larger group of researchers. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Computer Software,Longitudinal Studies,Simulation,Structural Equation Modeling,Time Series}
}

@misc{sorensenModelingCyclesTrends2025,
  title = {Modeling {{Cycles}}, {{Trends}} and {{Time-Varying Effects}} in {{Dynamic Structural Equation Models}} with {{Regression Splines}}},
  author = {S{\o}rensen, {\O}ystein and McCormick, Ethan M.},
  year = {2025},
  month = jan,
  publisher = {OSF},
  doi = {10.31234/osf.io/2ajpt},
  abstract = {Intensive longitudinal data with a large number of timepoints per individual are becoming increasingly common. Such data allow going beyond the classical growth model situation and studying population effects and individual variability not only in trends over time but also in autoregressive effects, cross-lagged effects, and the noise term. Dynamic structural equation models (DSEMs) have become very popular for analyzing intensive longitudinal data. However, when the data contain trends, cycles, or time-varying predictors which have nonlinear effects on the outcome, DSEMs require the practitioner to specify the correct parametric form of the effects, which may be challenging in practice. In this paper we show how to alleviate this issue by introducing regression splines which are able to flexibly learn the underlying function shapes. Our main contribution is thus a building block to the DSEM modeler's toolkit, and we discuss smoothing priors and hierarchical smooth terms using the special cases of two-level lag-1 autoregressive and vector autoregressive models as examples. We illustrate in simulation studies how ignoring nonlinear trends may lead to biased parameter estimates, and then show how to use the proposed framework to model weekly cycles and long-term trends in diary data on alcohol consumption and perceived stress.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {DSEM,intensive longitudinal data,regression splines,smoothing,Stan.}
}

@article{woodFastStableRestricted2011,
  title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
  author = {Wood, Simon N.},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {1},
  pages = {3--36},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  abstract = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton--Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
  copyright = {{\copyright} 2010 Royal Statistical Society},
  langid = {english},
  keywords = {Adaptive smoothing,Generalized additive mixed model,Generalized additive model,Generalized cross-validation,Marginal likelihood,Model selection,Penalized generalized linear model,Penalized regression splines,Restricted maximum likelihood,Scalar on function regression,Stable computation}
}
